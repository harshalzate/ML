{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP4IYszg0Ha7qN2eVJ/XQ50"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"J8ZXfWnXm5U5"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"l234lwt6tr2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Colab_Files_Data/NLP/IMDB.csv')\n","df.sample(5)"],"metadata":{"id":"Wn4Hmsl2nP7p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1 Lower case\n","df['review'] = df['review'].str.lower()"],"metadata":{"id":"xhFe-QSXng11"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2 Removing HTML tags\n","import re\n","def remove_html_tags(text):\n","  pattern=re.compile('<.*?>')\n","  return pattern.sub(r'',text)\n","df['review']=df['review'].apply(remove_html_tags)"],"metadata":{"id":"0RJHxi1hqzwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3 remove URL\n","def remove_url(text):\n","  pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","  return pattern.sub(r'',text)\n","df['review'] = df['review'].apply(remove_url)"],"metadata":{"id":"Huk8LI7Qrl4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4 Remove Puncutations\n","import string\n","exclude=string.punctuation\n","\n","def remove_punc(text):\n","  for char in exclude:\n","    return text.replace(char,'')\n","df['review'] = df['review'].apply(remove_punc)"],"metadata":{"id":"EadHhfEYuHX9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fast method compared to upper one\n","def remove_punctuations(text):\n","  return text.translate(str.maketrans('','',exclude))\n","df['review'] = df['review'].apply(remove_punctuations)"],"metadata":{"id":"n_E9-mdDvRMi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5 Chat word treatments\n","import pandas as pd\n","chat_word=pd.read_csv('Chat_Words.csv')\n","\n","# Assuming you have the DataFrame 'chat_word'\n","chat_word = pd.read_csv('Chat_Words.csv')\n","\n","def chat_words_processing(text):\n","    l = []\n","    for w in text.split():\n","        if w in chat_word['Abbreviation'].to_list():\n","            # Get the first element of the 'Meaning' column as a string\n","            meaning_str = str(chat_word.loc[chat_word['Abbreviation'] == w, 'Meaning'].values[0])\n","            l.append(meaning_str)\n","        else:\n","            l.append(w)\n","    return ' '.join(l)\n","\n","# Assuming 'df' is your DataFrame and 'review' is the column you want to process\n","df['review'] = df['review'].apply(chat_words_processing)"],"metadata":{"id":"2V0COBFDxA85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6 Spelling Mistakes\n","from textblob import TextBlob\n","def Spell_Mistake(text):\n","  textblb=TextBlob(text)\n","  return textblb.correct().string\n","df['review']=df['review'].apply(Spell_Mistake)"],"metadata":{"id":"XqoWoE4l_UO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7 Remove Stopwords\n","import nltk\n","nltk.download('stopwords')\n","stopwords=stopwords.words('english')\n","\n","def Remove_stopwords(text):\n","  l=[]\n","  for i in text.split():\n","    if i not in stopwords:\n","      l.append(i)\n","  return l\n","df['review'] = df['review'].apply(Remove_stopwords)"],"metadata":{"id":"XXw1dmYPB8m8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 8 Emojis Handling\n","# Either remove or replace with it's meaning\n","\n","pip install emoji\n","import emoji\n","import re\n","# 1 Replacement\n","def remove_emojis(text):\n","    # Remove emojis using the emoji library\n","    text_without_emojis = emoji.get_emoji_regexp().sub(u'', text)\n","\n","    # Additional removal of variations of emoji representations\n","    text_without_emojis = re.sub(r':[^:]+:', '', text_without_emojis)\n","\n","    return text_without_emojis\n","\n","# 2 Replacement\n","\n","import emoji\n","def replace_emojis_with_text_representation(text):\n","    # Replace emojis with their text representation using the demojize function from the emoji module\n","    text_with_text_representation = emoji.demojize(text)\n","\n","    return text_with_text_representation\n","\n","# Example usage\n","text_with_emojis = \"Hello üòä, how are you? üåç\"\n","text_with_text_representation = replace_emojis_with_text_representation(text_with_emojis)\n","\n","print(\"Original text:\", text_with_emojis)\n","print(\"Text with emoji text representation:\", text_with_text_representation)\n"],"metadata":{"id":"Dwh5W5hIQ2wm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 9 Tokenization   == Breaking down into smaller parts\n","# sentence level\n","# word level\n","# Normal split,regex and nltk can be used\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')    ## This is the tokenizer model which is pre trained which does word_tokenize and sent_tokenize\n","sent_1 = '''Harshal Ramesh Zate. He is the strongest Motherfuckers in the world.'''\n","sent_tokenize(sent_1)\n","sent2=''' Harshal's physique is out of the world. He is the Greek god '''\n","word_tokenize(sent2)"],"metadata":{"id":"xiYM_IloSGBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install spacy"],"metadata":{"id":"U1udNSzBXouk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 9 using spacy\n","import spacy\n","sent2=''' Harshal's physique is out of the world. He is the Greek god '''\n","nlp = spacy.load('en_core_web_sm')\n","[i for i in nlp(sent2)]"],"metadata":{"id":"wCyybNAmY4qz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 10 Stemming getting the words into it's root form\n","# There are different stemmers in nltk which were build by linguistic experts we'll use one of which\n","# porterstemmer\n","from nltk.stem import PorterStemmer\n","porter_stemmer = PorterStemmer()\n","text='''Harshal is having the time of his life, he's the strongest man alive, He's probably the strongest guy in the world mentally'''\n","[porter_stemmer.stem(i) for i in word_tokenize(text)]"],"metadata":{"id":"TsmtiqOlZiQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#11 Lemetization (difference from stemming is it reduces the word to roots form only but a meaningfule english word only unlike\n","                  # stemming which removes the unecessary parts which sometimes leaves us with words which are not meaningfule where as\n","                  # lemmetization finds the meanigfule word after convering into root form. usefule purticulary when you have to display the word back )\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('wordnet')\n","from nltk.tokenize import word_tokenize\n","lemmatizer = WordNetLemmatizer()\n","\n","text='''Harshal is having the time of his life, he's the strongest man alive, He's IS the strongest guy in the world mentally'''\n","[lemmatizer.lemmatize(i) for i in word_tokenize(text)]\n","\n","# see the difference in probably"],"metadata":{"id":"8pbDT5Aztr3T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text Representation"],"metadata":{"id":"wbLhB3z8NsUB"}},{"cell_type":"code","source":["# BOW\n","from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"4SbhWb2-xMsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","temp_df=pd.DataFrame([['people watch campusx',1],\n","                     ['campusx watch campusx',0],\n","                     ['people write comment',1],\n","                     ['campusx write comment',0]],columns=['text','output'])\n","temp_df"],"metadata":{"id":"Hbh4ZTmGRyMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorizor = CountVectorizer()\n","vectorizor.fit_transform(temp_df['text'])\n","feature_names = vectorizor.get_feature_names_out()\n","X_array = X.toarray()\n","vectorizor.vocabulary_"],"metadata":{"id":"_GHrYh98N_rx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# N-Gram\n","vectorizor = CountVectorizer(ngram_range=(2,2))\n","vectorizor.fit(temp_df['text'])\n","X_array = X.toarray()\n","vectorizor.vocabulary_"],"metadata":{"id":"sKuuVMIfTTvP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text Classification"],"metadata":{"id":"L_egFFdirL5o"}}]}