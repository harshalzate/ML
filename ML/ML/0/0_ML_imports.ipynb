{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNjC5fQyAEJv04m0EH2rBf7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Data genration\n","from sklearn.datasets import make_regression\n","import pandas as pd\n","X,y= make_regression(n_samples=100,n_features=2,noise=10,random_state=42)\n","# return the two array first for the X with dimension being the equal to n_features and len equal to the n_samples\n","# where as second being the target with the length equal to n_samples"],"metadata":{"id":"RHRjrCXKWOQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjpZbhuWVV85"},"outputs":[],"source":["#       -----------  model_selection  ----------------\n","# 1 train_test_split\n","from sklearn.model_selection import train_test_split\n","X_train,x_test,Y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n","#2 cross_validation\n","#3 Hyper parameter tuning\n","#4"]},{"cell_type":"code","source":["# -------------------------- preprocessing -----------------\n","#1 PolynomialFeatures\n","from sklearn.preprocessing import PolynomialFeatures\n","poly = PolynomialFeatures(2)\n","x_transfrom=poly.fit_transform(X)"],"metadata":{"id":"FBdtSPcVdMaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--------------------Feature selection------------------------\n","#   A: Filter based methods\n","# 1. varience threshold\n","from sklearn.feature_selection import VarianceThreshold\n","# 2. Correlation\n","(X_train).corr()\n","# Annova\n","from sklearn.feature_selection import f_classif\n","from sklearn.feature_selection import SelectKBest\n","\n","# Wrapper methods\n","# 1. Exhaustive feature selection\n","from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n","lr = LogisticRegression()\n","sel = EFS(lr, max_features=4, scoring='accuracy', cv=5)\n","sel.fit(X_train,y_train)\n","\n","#2 Sequential\n","from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n","# 3. RFE\n","from sklearn.feature_selection import RFE\n"],"metadata":{"id":"9kancdD8JFon"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature Extraction\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE"],"metadata":{"id":"rKeDgsIb_RAA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# imblanced data\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.over_sampling import SMOTE\n","from imblearn.ensemble import BalancedRandomForestClassifier\n","from sklearn.linear import LinearRegression\n","# class weights to the\n","LinearRegression(class_weight={0:50,1:1})"],"metadata":{"id":"H4hvJ1G8oHGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ZZZ# -------------------- Metrics -------------------------------\n","# 1 regression\n","from sklearn.metrics import mean_absolute_erros,mean_squared_error, r2_score\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score, max_error, mean_squared_log_error, median_absolute_error\n","\n","# 2 Classification\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import confusion_matrix, roc_auc_score, auc\n","# 3 error\n","from sklearn.metrics import mean_absolute_erros,mean_squared_error, r2_score\n","#\n","from sklearn.metrics import"],"metadata":{"id":"EWOPKsUeepR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ------------------ linear_model ----------------------\n","#1 LinearRegression\n","from sklearn.linear_model import LinearRegression\n","# polyomial regression\n","from sklearn.preprocessing import PolynomialFeatures\n","#2 Ridge\n","from sklearn.linear_model import Ridge\n","#3 Lasso\n","from sklearn.linear_model import Lasso\n","#4 ElasticNet\n","from sklearn.linear_model import ElasticNet\n","#5 Logistic Regression\n","from sklearn.linear_model import LogisticRegression\n","#6 SGDRgressor\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.linear_model import SGDClassifier\n","#7 Bayesian Regression\n","from sklearn.linear_model import BayesianRidge\n","\n","## ------------------Knn----------------------------------\n","from sklearn.neighbors import KNeighborsClassifier\n"],"metadata":{"id":"oaTKaKRKVgUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# XGBoost\n","!pip install XGBoost as xgb"],"metadata":{"id":"KBjcKt2QkOzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb."],"metadata":{"id":"WGkR1LxnkXgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KU2bKl2-E2GV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NLP\n","\n","# spelling\n","from textblob import TextBlob\n","def Spell_Mistake(text):\n","  textblb=TextBlob(text)\n","  return textblb.correct().string\n","\n","# stopwords\n","import nltk\n","nltk.download('stopwords')\n","stopwords=stopwords.words('english')\n","# Tokenization\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')    ## This is the tokenizer model which is pre trained which does word_tokenize and sent_tokenize\n","sent_1 = '''Harshal Ramesh Zate. He is the strongest Motherfuckers in the world.'''\n","sent_tokenize(sent_1)\n","sent2=''' Harshal's physique is out of the world. He is the Greek god '''\n","word_tokenize(sent2)\n","# with spacy\n","\n","import spacy\n","sent2=''' Harshal's physique is out of the world. He is the Greek god '''\n","nlp = spacy.load('en_core_web_sm')\n","[i for i in nlp(sent2)]\n","\n","# Stemming\n","text='''Harshal is having the time of his life, he's the strongest man alive, He's IS the strongest guy in the world mentally'''\n","from nltk.stem import PorterStemmer\n","stemm=PorterStemmer()\n","for i in text.split():\n","  stemm.stem(i)\n","\n","# Lemmetization\n","from nltk.stem import WordNetLemmatizer\n","import nltk\n","nltk.download('wordnet')\n","lem=WordNetLemmatizer()\n","text='''Harshal is having the time of his life, he's the strongest man alive, He's IS the strongest guy in the world mentally'''\n","for i in text.split():\n","  print(lem.lemmatize(i))"],"metadata":{"id":"VZ6MLNJhE2U7"},"execution_count":null,"outputs":[]}]}